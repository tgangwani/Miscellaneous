{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code: Python3 that implements mistake-bound 'Perceptron' algorithm for multi-class\n",
    "classification. It uses Kesler's construction as outlined in\n",
    "http://l2r.cs.illinois.edu/~danr/Teaching/CS446-15/Lectures/07-LecMulticlass.pdf.\n",
    "The update rule is 'conservative', the definition of which is provided on\n",
    "slide-32 of this pdf.\n",
    "\n",
    "Dataset: http://www.start.umd.edu/gtd/\n",
    "\"The Global Terrorism Database (GTD) is an open-source database including\n",
    "information on terrorist events around the world from 1970 through 2014 (with\n",
    "annual updates planned for the future).\"\n",
    "\n",
    "Problem: Using 'features' associated with an incident like location, weapon-type etc.\n",
    "to predict the group (outfit) associated or responsible for the incident. The\n",
    "current implementation uses 25 features out of many provided at the source link.\n",
    "\n",
    "Input: The program expects the file 'gtd.data' in the run folder. info.log, eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import logging\n",
    "from enum import Enum                 \n",
    "\n",
    "class classifier(object):\n",
    "  \"\"\"\n",
    "  based class from which each classification algorithm is derived\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def test(self):\n",
    "    \"\"\"\n",
    "    purely virutal method\n",
    "    \"\"\"                      \n",
    "    raise NotImplementedError(':test')\n",
    "\n",
    "  def train(self):\n",
    "    \"\"\"\n",
    "    purely virutal method\n",
    "    \"\"\"                      \n",
    "    raise NotImplementedError(':train')\n",
    "\n",
    "  def eval(self):\n",
    "    \"\"\"\n",
    "    purely virutal method\n",
    "    \"\"\"                      \n",
    "    raise NotImplementedError(':eval')\n",
    "\n",
    "class dataset():\n",
    "  \"\"\"\n",
    "  class to store information related to input/output data\n",
    "  \"\"\"\n",
    "  def __init__(self, filename):\n",
    "    if not os.path.exists(filename):\n",
    "      raise LookupError('Input file not found %s', filename)\n",
    "\n",
    "    self.labelMap = {} # stores maps {label_names -> whole_numbers}            \n",
    "    self.reverseLabelMap = {} # stores maps {whole_numbers -> label_names}\n",
    "    self.features = []\n",
    "    self.labels = []\n",
    "\n",
    "  def splitData(self, testSplit : 'float'):\n",
    "    \"\"\"\n",
    "    uses the sklearn.cross_validation module to perform a random splitting of\n",
    "    the input data\n",
    "    \"\"\"\n",
    "    self.features_train, self.features_test, self.labels_train, self.labels_test = \\\n",
    "      train_test_split(self.features, self.labels, test_size=testSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gtd(dataset):\n",
    "  \"\"\"\n",
    "  class for the gtd dataset\n",
    "  \"\"\"\n",
    "  def __init__(self, filename):\n",
    "\n",
    "    # typedefs ('float' instead of 'int', since pandas has trouble when\n",
    "    # dtype=int has Nan in the input file)\n",
    "    categorical = np.float32 \n",
    "    boolean = np.float32 \n",
    "\n",
    "    # dictionary of important features (manually selected) to start analysis\n",
    "    # we could use PCA/RandomForest/other-techniques to further reduce dimensionality \n",
    "    self.relevantFeatures = {\n",
    "        'eventid':int,\n",
    "        'extended':boolean,\n",
    "        'country':categorical,\n",
    "        'region':categorical,\n",
    "        'latitude':np.float32,\n",
    "        'longitude':np.float32,\n",
    "        'attacktype1':categorical,\n",
    "        'success':boolean,\n",
    "        'suicide':boolean,\n",
    "        'crit1':boolean,\n",
    "        'crit2':boolean,\n",
    "        'crit3':boolean,\n",
    "        'multiple':boolean,\n",
    "        'targtype1':categorical,\n",
    "        'natlty1':categorical,\n",
    "        'guncertain1':boolean,\n",
    "        'claimed':boolean,\n",
    "        'weaptype1':categorical,\n",
    "        'nkill':np.float32,\n",
    "        'nwound':np.float32,\n",
    "        'property':boolean,\n",
    "        'ishostkid':boolean,\n",
    "        'INT_LOG':boolean,\n",
    "        'INT_IDEO':boolean,\n",
    "        'INT_MISC':boolean,\n",
    "        'INT_ANY':boolean,\n",
    "        'gname':str\n",
    "    }\n",
    "    \n",
    "    super().__init__(filename)\n",
    "    self.readFile(filename)\n",
    "\n",
    "  def readFile(self, filename):\n",
    "    # read data into a data-frame\n",
    "    data = pd.read_csv(filename, header=0, skiprows=0,\\\n",
    "        sep=r\",\", na_values=\" NaN\", dtype=self.relevantFeatures, low_memory=False)\n",
    "\n",
    "    #select relevant features\n",
    "    data = data[list(self.relevantFeatures.keys())] \n",
    "\n",
    "    # ad-hoc : dropping the rows with a NaN. We could do something intelligent\n",
    "    # here like deducing the hidden/missing variables\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # separate the dataframe to be predicted\n",
    "    evalFrame = data[data['gname']=='Unknown']\n",
    "\n",
    "    # fill the labels (group-name) and features\n",
    "    data = data[data['gname']!='Unknown']\n",
    "    self.labels = list(data['gname']) \n",
    "    data.drop(['gname', 'eventid'], axis=1, inplace=True)\n",
    "    self.features=np.array(data)\n",
    "    \n",
    "    # add a column of 1's to the feature vector since we'll absort theta into the\n",
    "    # w-matrix\n",
    "    self.features = np.append(self.features, np.ones([self.features.shape[0], 1]), axis=1)\n",
    "\n",
    "    # fill maps\n",
    "    for label, index in zip(set(self.labels), range(len(set(self.labels)))):\n",
    "      self.labelMap[index] = label\n",
    "      self.reverseLabelMap[label] = index\n",
    "\n",
    "    # change the labels on input data to whole numbers\n",
    "    for index in range(len(self.labels)):\n",
    "      self.labels[index] = self.reverseLabelMap[self.labels[index]]\n",
    "\n",
    "    self.num_labels = self.labelMap.__len__() # number of 'unique' labels\n",
    "    self.features_dim = self.features.shape[1] # dimensionality of data\n",
    "\n",
    "    # fill the features for evaluation using the 'evalFrame' dataframe that was\n",
    "    # created earlier\n",
    "    self.eventids_eval = list(evalFrame['eventid'])  # event-ids for the incidents with unknown group\n",
    "    evalFrame.drop(['gname', 'eventid'], axis=1, inplace=True)\n",
    "    self.features_eval = np.array(evalFrame)\n",
    "\n",
    "    # add a column of 1's to the feature vector \n",
    "    self.features_eval = np.append(self.features_eval, np.ones([self.features_eval.shape[0], 1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.FileHandler('info.log', mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class perceptron(classifier):\n",
    "  \"\"\"\n",
    "  multi-class perceptron \n",
    "  \"\"\"\n",
    "  def __init__(self, d : 'dataset', **kwds):\n",
    "    super().__init__()                    \n",
    "    assert isinstance(d, dataset)\n",
    "\n",
    "    # the weight matrix is rxd, r=#labels, d=featureDimension+1\n",
    "    self.weights = np.zeros([d.num_labels, d.features_dim])\n",
    "    self.learningRate = kwds['learningRate']\n",
    "    self.maxIterations = kwds['maxIterations']\n",
    "    self.dataset = d\n",
    "  \n",
    "  def eval(self):\n",
    "    logger.info('Begin evaluating Perceptron')\n",
    "    fd = open('eval.txt', 'w')\n",
    "\n",
    "    for _id, feature in zip(self.dataset.eventids_eval, \\\n",
    "        self.dataset.features_eval):\n",
    "      prediction = self.dataset.labelMap[self.getLabel(feature)]\n",
    "      fd.write(\"Eventid:\" + str(_id) +  \"\\tPredicted group:\" + prediction+\"\\n\")\n",
    "\n",
    "    fd.close()\n",
    "\n",
    "  def test(self):\n",
    "    logger.info('Begin testing Perceptron')\n",
    "\n",
    "    mistakes = int(0)\n",
    "    for feature, label in zip(self.dataset.features_test, self.dataset.labels_test):\n",
    "      prediction = self.getLabel(feature)\n",
    "      if prediction != label:\n",
    "        mistakes = mistakes + 1\n",
    "\n",
    "    print('Test accuracy={:f}'.format(1-mistakes/len(self.dataset.labels_test)))\n",
    "\n",
    "  def getLabel(self, feature) -> 'label':\n",
    "    prediction = np.argmax(np.dot(self.weights, feature))\n",
    "    return prediction\n",
    "\n",
    "  def train(self):\n",
    "    logger.info('Begin training Perceptron')\n",
    "    logger.info(\"#Training Samples=%i\" %len(self.dataset.labels_train))\n",
    " \n",
    "    iterations = int(0)\n",
    "    mistakes = int(0)\n",
    "\n",
    "    while True:\n",
    "      \n",
    "      iterations = iterations + 1 \n",
    "      if iterations == self.maxIterations:\n",
    "        logger.info('Reached maximum iteration count. Terminating training')\n",
    "        print('Training accuracy={:f}'.format(1-mistakes/len(self.dataset.labels_train)))\n",
    "        break\n",
    "\n",
    "      mistakes = 0  # reset\n",
    "      for feature, label in zip(self.dataset.features_train, self.dataset.labels_train):\n",
    "        prediction = self.getLabel(feature)\n",
    "        \n",
    "        # mistake-bound perceptron algorithm using Keslers's construction. If\n",
    "        # case of a misprediction, we increase weights corresponding to the\n",
    "        # correct label, and decrease weights corresponding to the predicted\n",
    "        # label\n",
    "        if prediction != label:\n",
    "          u_mat = np.zeros(self.weights.shape)\n",
    "          u_mat[label,:] = self.learningRate*feature\n",
    "          u_mat[prediction,:] = (-1.)*self.learningRate*feature\n",
    "           \n",
    "          # update the weights matrix \n",
    "          self.weights = np.add(self.weights, u_mat)\n",
    "          mistakes = mistakes + 1\n",
    "      \n",
    "      if not mistakes:\n",
    "        logger.info('Training complete. No mistakes on training data')\n",
    "        break\n",
    "      else:\n",
    "        logger.debug(\"Completed iteration %i  #Mistakes %i\" %(iterations, mistakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class algo(str, Enum):\n",
    "  \"\"\"\n",
    "  insert all supported algorithms here\n",
    "  \"\"\"\n",
    "  perceptron = 'perceptron'\n",
    "\n",
    "  @property\n",
    "  def method(self):\n",
    "    return algo.get_method(self) \n",
    "\n",
    "  @classmethod\n",
    "  def get_method(cls, a) -> 'class object':\n",
    "    if a is cls.perceptron:\n",
    "      return perceptron\n",
    "    else:\n",
    "      raise KeyError(\"Unrecognized algorithm %s\" % a)\n",
    "\n",
    "def run(filename : 'str', algorithm : 'str', kwds):\n",
    "  d = gtd(filename)\n",
    "  d.splitData(kwds['testSplit'])\n",
    "                          \n",
    "  # select the algorithm to run and train                        \n",
    "  sel = algo(algorithm).method(d, **kwds)\n",
    "  sel.train()\n",
    "  sel.test()\n",
    "  sel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'gtd.data' # expects csv          \n",
    "run(filename, 'perceptron', {'testSplit':0.2, 'learningRate':0.005, 'maxIterations':10000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Prel. results):\n",
    "The following results are using the 2011-14 data-file provided at the source\n",
    "link. The perceptron algorithm is run on labeled data (with assigned groups -\n",
    "19075 out of 42373). 80-20 cross-validation is used. The accuracies seen after 10000\n",
    "iterations and a learning rate of 0.005 are:\n",
    "\n",
    "Training accuracy=0.773\n",
    "Test accuracy=0.747\n",
    "\n",
    "The file 'eval.txt' contains the predictions for the data that was not labeled in\n",
    "the input (23298 out of 42373). The incidents are identified by the eventid field provided in source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
